"""
Tool Output Sanitization Framework

This module provides comprehensive sanitization of tool outputs to prevent prompt injection
attacks and ensure safe integration with LLM contexts.

Features:
- ANSI code stripping
- Control character removal
- Whitespace normalization
- Instruction injection detection
- Taint marking system
- OWASP pattern filtering
"""

import re
import logging
from typing import Any, Dict, List, Optional, Union
from enum import Enum
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)


class TaintLevel(Enum):
    """Taint levels for marking content sources."""

    SYSTEM = "SYSTEM"  # âœ“SYSTEM - Generated by system/agent
    EXTERNAL = "EXTERNAL"  # âš ï¸EXTERNAL - Retrieved from external source
    SEARCH = "SEARCH"  # ðŸ”SEARCH - Retrieved from search/knowledge base
    USER = "USER"  # ðŸ‘¤USER - User-provided input


@dataclass
class TaintedString:
    """
    String with taint metadata to track content source and safety.

    Attributes:
        content: The sanitized content
        taint_level: Source taint level
        original_length: Original content length before sanitization
        sanitization_applied: List of sanitization steps applied
        detected_threats: List of detected threat patterns
    """

    content: str
    taint_level: TaintLevel
    original_length: int = 0
    sanitization_applied: List[str] = field(default_factory=list)
    detected_threats: List[str] = field(default_factory=list)

    def __str__(self) -> str:
        """Return content with taint marker prefix."""
        markers = {
            TaintLevel.SYSTEM: "âœ“SYSTEM",
            TaintLevel.EXTERNAL: "âš ï¸EXTERNAL",
            TaintLevel.SEARCH: "ðŸ”SEARCH",
            TaintLevel.USER: "ðŸ‘¤USER",
        }
        marker = markers.get(self.taint_level, "")
        return f"{marker}\n{self.content}" if marker else self.content

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "content": self.content,
            "taint_level": self.taint_level.value,
            "original_length": self.original_length,
            "sanitization_applied": self.sanitization_applied,
            "detected_threats": self.detected_threats,
        }


class SanitizationConfig:
    """Configuration for sanitization behavior."""

    def __init__(
        self,
        strip_ansi: bool = True,
        remove_control_chars: bool = True,
        normalize_whitespace: bool = True,
        detect_injections: bool = True,
        max_length: Optional[int] = None,
        log_detections: bool = True,
    ):
        self.strip_ansi = strip_ansi
        self.remove_control_chars = remove_control_chars
        self.normalize_whitespace = normalize_whitespace
        self.detect_injections = detect_injections
        self.max_length = max_length
        self.log_detections = log_detections


# OWASP injection patterns
INJECTION_PATTERNS = [
    # Instruction injection
    r"(?i)(ignore|forget|disregard|skip)\s+(previous|prior|earlier|above|all)\s+(instructions?|commands?|directives?)",
    r"(?i)(system|assistant|user|role):\s*",
    r"(?i)(you are|act as|pretend|roleplay|simulate)",
    r"(?i)(new instructions?|override|replace)\s+(instructions?|system|prompt)",
    # Delimiter injection
    r"<\|(system|assistant|user|endoftext)\|>",
    r"```(system|assistant|user|instructions)",
    r"---(system|assistant|user|instructions)---",
    # Encoded injections
    r"(?i)(base64|hex|url|unicode)\s*(encoded|decoded)?\s*(instructions?|commands?|payload)",
    # Jailbreak attempts
    r"(?i)(jailbreak|dan|do anything now|developer mode|god mode)",
    r"(?i)(bypass|circumvent|override)\s+(safety|security|restrictions?|filters?)",
    # Control sequences
    r"\x1b\[[0-9;]*m",  # ANSI escape sequences
    r"[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]",  # Control characters
]

# Compiled regex patterns for performance
COMPILED_PATTERNS = [re.compile(pattern) for pattern in INJECTION_PATTERNS]


def strip_ansi_codes(text: str) -> str:
    """
    Remove ANSI escape codes from text.

    Args:
        text: Input text potentially containing ANSI codes

    Returns:
        Text with ANSI codes removed
    """
    # Remove ANSI escape sequences
    ansi_escape = re.compile(r"\x1b\[[0-9;]*m")
    return ansi_escape.sub("", text)


def remove_control_chars(text: str) -> str:
    """
    Remove control characters except newlines and tabs.

    Args:
        text: Input text potentially containing control characters

    Returns:
        Text with control characters removed
    """
    # Keep newlines (\n), carriage returns (\r), and tabs (\t)
    # Remove other control characters (0x00-0x1F except 0x09, 0x0A, 0x0D)
    # and DEL (0x7F) and extended control (0x80-0x9F)
    cleaned = ""
    for char in text:
        code = ord(char)
        if (
            code == 0x09  # Tab
            or code == 0x0A  # Newline
            or code == 0x0D  # Carriage return
            or (code >= 0x20 and code < 0x7F)  # Printable ASCII
            or (code >= 0xA0)  # Extended characters
        ):
            cleaned += char
    return cleaned


def normalize_whitespace(text: str) -> str:
    """
    Normalize excessive whitespace while preserving structure.

    Args:
        text: Input text with potentially excessive whitespace

    Returns:
        Text with normalized whitespace
    """
    # Replace multiple spaces with single space
    text = re.sub(r" +", " ", text)
    # Replace multiple newlines (more than 2) with double newline
    text = re.sub(r"\n{3,}", "\n\n", text)
    # Remove trailing whitespace from lines
    lines = [line.rstrip() for line in text.split("\n")]
    return "\n".join(lines)


def detect_instruction_injection(text: str) -> List[str]:
    """
    Detect instruction injection patterns in text.

    Args:
        text: Input text to check

    Returns:
        List of detected threat patterns
    """
    detected = []
    for i, pattern in enumerate(COMPILED_PATTERNS):
        matches = pattern.findall(text)
        if matches:
            threat_type = INJECTION_PATTERNS[i][:50]  # Truncate for logging
            detected.append(f"Pattern {i}: {threat_type}")
            logger.warning(
                f"Detected potential injection pattern: {threat_type}",
                extra={"pattern_index": i, "matches_count": len(matches)},
            )
    return detected


def sanitize_text(
    text: str,
    config: Optional[SanitizationConfig] = None,
    taint_level: TaintLevel = TaintLevel.EXTERNAL,
) -> TaintedString:
    """
    Sanitize text content with comprehensive filtering.

    Args:
        text: Input text to sanitize
        config: Sanitization configuration (uses defaults if None)
        taint_level: Taint level to mark content with

    Returns:
        TaintedString with sanitized content and metadata
    """
    if config is None:
        config = SanitizationConfig()

    original_length = len(text)
    sanitized = text
    applied_steps = []
    detected_threats = []

    # Strip ANSI codes
    if config.strip_ansi:
        sanitized = strip_ansi_codes(sanitized)
        applied_steps.append("strip_ansi")

    # Remove control characters
    if config.remove_control_chars:
        sanitized = remove_control_chars(sanitized)
        applied_steps.append("remove_control_chars")

    # Normalize whitespace
    if config.normalize_whitespace:
        sanitized = normalize_whitespace(sanitized)
        applied_steps.append("normalize_whitespace")

    # Detect injection patterns
    if config.detect_injections:
        threats = detect_instruction_injection(sanitized)
        if threats:
            detected_threats = threats
            applied_steps.append("injection_detection")
            # Remove detected injection patterns
            for pattern in COMPILED_PATTERNS:
                sanitized = pattern.sub("", sanitized)
            applied_steps.append("injection_removal")

    # Apply length limit
    if config.max_length and len(sanitized) > config.max_length:
        sanitized = sanitized[: config.max_length] + "... [truncated]"
        applied_steps.append("truncate")

    # Log detections if configured
    if config.log_detections and detected_threats:
        logger.warning(
            f"Sanitization detected {len(detected_threats)} threats",
            extra={"threats": detected_threats, "original_length": original_length},
        )

    return TaintedString(
        content=sanitized,
        taint_level=taint_level,
        original_length=original_length,
        sanitization_applied=applied_steps,
        detected_threats=detected_threats,
    )


def sanitize_tool_output(
    output: Union[str, Dict[str, Any], List[Any]],
    source: str = "unknown",
    config: Optional[SanitizationConfig] = None,
    taint_level: TaintLevel = TaintLevel.EXTERNAL,
) -> Union[str, Dict[str, Any], List[Any]]:
    """
    Sanitize tool output of various types.

    Args:
        output: Tool output (string, dict, or list)
        source: Source identifier for logging
        config: Sanitization configuration
        taint_level: Taint level to apply

    Returns:
        Sanitized output of same type
    """
    if isinstance(output, str):
        tainted = sanitize_text(output, config, taint_level)
        return str(tained)  # Returns with taint marker

    elif isinstance(output, dict):
        sanitized = {}
        for key, value in output.items():
            if isinstance(value, str):
                tainted = sanitize_text(value, config, taint_level)
                sanitized[key] = str(tained)
            elif isinstance(value, (dict, list)):
                sanitized[key] = sanitize_tool_output(value, source, config, taint_level)
            else:
                sanitized[key] = value
        return sanitized

    elif isinstance(output, list):
        return [
            sanitize_tool_output(item, source, config, taint_level) for item in output
        ]

    return output


def mark_external(content: str) -> TaintedString:
    """Mark content as external source."""
    return sanitize_text(content, taint_level=TaintLevel.EXTERNAL)


def mark_system(content: str) -> TaintedString:
    """Mark content as system-generated."""
    return sanitize_text(content, taint_level=TaintLevel.SYSTEM)


def mark_search(content: str) -> TaintedString:
    """Mark content as search result."""
    return sanitize_text(content, taint_level=TaintLevel.SEARCH)


def mark_user(content: str) -> TaintedString:
    """Mark content as user-provided."""
    return sanitize_text(content, taint_level=TaintLevel.USER)



